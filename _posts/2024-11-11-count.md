---
title: "The humble count"
date: 2024-11-11
---

Who would have thought that the humble count operation can be so slow on a row store? I certainly didn't before I started to dive into databases, but it all makes sense actually.

It all boils down to count being an aggregate operations, which is an inefficient IO pattern on row stores. 

The ways to make it work are:
1. *Use an index only scan:* Because indexes are contigous in pages, it practically becomes a little column storage that is faster than scanning the actual records. It will have it's limits if there are NULLs in the table because they are not indexed, or there are multi-column filters for the count. 
2. *Use a trigger:* A synchronous is a naive approach to update a counter, but it has scaling limitations due to lock syncs. 
3. *Materialised views:* Update a materialised view every few mintes with the count of the tables. It is a workable async approach.
4. *Use row estimates:* If only an estimated value is needed, then row estimates might be a good approximation. Be aware that it needs regular and correct analyzes to make it work properly.
5. *Outside column store:* If there is a DWH around, it can be considered for those counts, adding complexity in database management and eventual consistency.
6. *Inside column store:* There is a new DuckDB extension to Postgres that might be worth looking at.
7. *HyperLogLog:* There are extensions for postgres to maintain a probabilistic approximation for table cardinality using a HLL data structure.
8. *Filters:* Look up Postgres filters extension ???

All these approaches have their trade-offs, which I should expand on at some point. 

But let me just leave here a few great resources that I found in the topic for later reference:
* https://www.citusdata.com/blog/2016/10/12/count-performance/?fbclid=IwZXh0bgNhZW0CMTEAAR0JeGqeP-pT8KJzf8U4XxgtiMQ9-xEMiLaMsqYe6tQpN2KSKch8ZoqZOuY_aem_3btjNmvmFM_is1Nlebpr1A
* https://wiki.postgresql.org/wiki/Slow_Counting
* https://postgres.fm/episodes/slow-count
